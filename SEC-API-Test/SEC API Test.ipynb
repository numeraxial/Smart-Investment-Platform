{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61befee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = \"9626ec2105ea082042fdda6db36f060dd228f2edc6229e8c5cd1cc3b2c673385\"\n",
    "API_KEY = \"f9170d7223a9d2ba983443847320aa658edcc210742ee811ca09e1ab0dd956c9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0771541",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from sec_api import QueryApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee73156",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31472218",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_api = QueryApi(api_key=API_KEY)\n",
    "\n",
    "# fetch all 10-Q and 10-K\n",
    "query = { \n",
    "    \"query\": {\n",
    "        \"query_string\": {\n",
    "            \"query\": \"(formType:\\\"10-Q\\\" OR formType:\\\"10-K\\\") AND ticker:\"+ticker\n",
    "        }\n",
    "    },\n",
    "    \"from\": \"0\",\n",
    "    \"size\": \"20\",\n",
    "    \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
    "}\n",
    "\n",
    "query_result = query_api.get_filings(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff5162",
   "metadata": {},
   "source": [
    "### get the accessioin number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "accession_numbers = []\n",
    "\n",
    "# extract accession numbers of each filing\n",
    "for filing in query_result['filings']:\n",
    "    accession_numbers.append(filing['accessionNo'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daff011",
   "metadata": {},
   "source": [
    "### get the XBLR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get XBRL-JSON for a given accession number\n",
    "def get_xbrl_json(accession_no):\n",
    "    '''\n",
    "    Helper function to get XBRL-JSON version of a filing by providing its accession number\n",
    "    '''\n",
    "    # XBRL-to-JSON converter API endpoint\n",
    "    xbrl_converter_api_endpoint = \"https://api.sec-api.io/xbrl-to-json\"\n",
    "\n",
    "    request_url = xbrl_converter_api_endpoint + \"?accession-no=\" + accession_no + \"&token=\" + API_KEY\n",
    "    \n",
    "    response_tmp = requests.get(request_url)\n",
    "    \n",
    "    return json.loads(response_tmp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49f992",
   "metadata": {},
   "source": [
    "### Merge two statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two statements into one statement\n",
    "# row indices of both statements have to be the same\n",
    "# statement_b represents the most recent statement.\n",
    "def merge_statements(statement_a, statement_b):\n",
    "    return statement_a.merge(statement_b,\n",
    "                             how=\"outer\",\n",
    "                             left_index=True,\n",
    "                             right_index=True,\n",
    "                             suffixes=('_left', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9d377",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean statement\n",
    "# drop duplicate columns (= column name ends with \"_left\"), drop key_0 column, drop columns with +5 NaNs\n",
    "def clean_data_statement(statement):\n",
    "    for column in statement:\n",
    "        \n",
    "        #column has more than 5 NaN values\n",
    "        is_nan_column = statement[column].isna().sum() > 5\n",
    "        \n",
    "        if column.endswith('_left') or column == 'key_0' or is_nan_column:\n",
    "            statement = statement.drop(column, axis=1)\n",
    "        \n",
    "    # rearrange columns so that first column represents first quarter\n",
    "    # e.g. 2018, 2019, 2020 - and not 2020, 2019, 2018\n",
    "    sorted_columns = sorted(statement.columns.values)\n",
    "    \n",
    "    return statement[sorted_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44961043",
   "metadata": {},
   "source": [
    "### Income Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939da0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert XBRL-JSON of income statement to pandas dataframe\n",
    "def get_income_statement(xbrl_json):\n",
    "    income_statement_store = {}\n",
    "    \n",
    "    # iterate over each US GAAP item in the income statement\n",
    "    for usGaapItem in xbrl_json['StatementsOfIncome']:\n",
    "        values = []\n",
    "        indicies = []\n",
    "        \n",
    "        for fact in xbrl_json['StatementsOfIncome'][usGaapItem]:\n",
    "            # only consider items without segment.\n",
    "            if 'segment' not in fact:\n",
    "                index = fact['period']['startDate'] + '-' + fact['period']['endDate']\n",
    "                # ensure no index duplicates are created\n",
    "                if index not in indicies:\n",
    "                    values.append(fact['value'])\n",
    "                    indicies.append(index)\n",
    "        \n",
    "        income_statement_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "        \n",
    "    income_statement = pd.DataFrame(income_statement_store)\n",
    "    # switch columns and rows so that US GAAP items are rows and each column header represents a data range\n",
    "    return income_statement .T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af93377",
   "metadata": {},
   "source": [
    "### Balance Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert XBRL-JSON of balance sheet to pandas dataframe\n",
    "def get_balance_sheet(xbrl_json):\n",
    "    balance_sheet_store = {}\n",
    "    \n",
    "    for usGaapItem in xbrl_json['BalanceSheets']:\n",
    "        values = []\n",
    "        indicies = []\n",
    "        \n",
    "        for fact in xbrl_json['BalanceSheets'][usGaapItem]:\n",
    "            # only consider items without segment.\n",
    "            if 'segment' not in fact:\n",
    "                index = fact['period']['instant']\n",
    "                \n",
    "                # avoid duplicate indices with same values\n",
    "                if index in indicies:\n",
    "                    continue\n",
    "                \n",
    "                # add 0 if value is nil\n",
    "                if \"value\" not in fact:\n",
    "                    values.append(0)\n",
    "                else:\n",
    "                    values.append(fact['value'])\n",
    "                    \n",
    "                indicies.append(index)\n",
    "            \n",
    "            balance_sheet_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "            \n",
    "    balance_sheet = pd.DataFrame(balance_sheet_store)\n",
    "    # switch columns and rows so that US GAAP items are rows and each column header represents a date instant\n",
    "    return balance_sheet.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f72863",
   "metadata": {},
   "source": [
    "### Cash Flow Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the cash flow statement\n",
    "def get_cash_flow_statement(xbrl_json):\n",
    "    cash_flows_store = {}\n",
    "    \n",
    "    for usGaapItem in xbrl_json['StatementsOfCashFlows']:\n",
    "        values = []\n",
    "        indicies = []\n",
    "        \n",
    "        for fact in xbrl_json['StatementsOfCashFlows'][usGaapItem]:\n",
    "            # only consider items without segment\n",
    "            if 'segment' not in fact:\n",
    "                #check if data instant or date range is present\n",
    "                if 'instant' in fact['period']:\n",
    "                    index = fact['period']['instant']\n",
    "                else:\n",
    "                    index = fact['period']['startDate'] + '-' + fact['period']['endDate']\n",
    "            \n",
    "                # avoid duplicate indicies with same values\n",
    "                if index in indicies:\n",
    "                    continue\n",
    "                \n",
    "                if \"value\" not in fact:\n",
    "                    values.append(0)\n",
    "                else:\n",
    "                    values.append(fact['value'])\n",
    "                \n",
    "                indicies.append(index)\n",
    "        \n",
    "        cash_flows_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "    \n",
    "    cash_flows = pd.DataFrame(cash_flows_store)\n",
    "    return cash_flows.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ceac80",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8ec92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "previous_income_statement_set = False\n",
    "income_statement_final = None\n",
    "\n",
    "previous_balance_sheet_set = False\n",
    "balance_sheet_final = None\n",
    "\n",
    "previous_cashflow_statement_set = False\n",
    "cashflow_statement_final = None\n",
    "\n",
    "for accession_no in accession_numbers[0:9]: # doesn't work with filings filed before 2017\n",
    "\n",
    "    # get XBRL-JSON of 10-Q or 10-K filing by accession number\n",
    "    xbrl_json_data = get_xbrl_json(accession_no)\n",
    "    \n",
    "    print(xbrl_json_data)\n",
    "    \n",
    "    # convert XBRL-JSON to a pandas dataframe\n",
    "    income_statement_uncleaned = get_income_statement(xbrl_json_data)\n",
    "    balance_sheet_uncleaned = get_balance_sheet(xbrl_json_data)\n",
    "    cashflow_statement_uncleaned = get_cash_flow_statement(xbrl_json_data)\n",
    "    \n",
    "    # clean the data\n",
    "    income_statement_cleaned = clean_data_statement(income_statement_uncleaned)\n",
    "    balance_sheet_cleaned = clean_data_statement(balance_sheet_uncleaned)\n",
    "    cashflow_statement_cleaned = clean_data_statement(cashflow_statement_uncleaned)\n",
    "    \n",
    "    # merge new income statement with previously generated income statement\n",
    "    if previous_income_statement_set and previous_balance_sheet_set and previous_cashflow_statement_set:\n",
    "        income_statement_final = clean_data_statement(merge_statements(income_statement_final, income_statement_cleaned))\n",
    "        balance_sheet_final = clean_data_statement(merge_statements(balance_sheet_final, balance_sheet_cleaned))\n",
    "        cashflow_statement_final = clean_data_statement(merge_statements(cashflow_statement_final, cashflow_statement_cleaned))\n",
    "    else:\n",
    "        income_statement_final = income_statement_cleaned\n",
    "        balance_sheet_final = balance_sheet_cleaned\n",
    "        cashflow_statement_final = cashflow_statement_cleaned\n",
    "        previous_income_statement_set = True\n",
    "        previous_balance_sheet_set = True\n",
    "        previous_cashflow_statement_set = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a14d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3344198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e6afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630b166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f71142",
   "metadata": {},
   "source": [
    "### New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_income_statement_set = False\n",
    "income_statement_final = None\n",
    "\n",
    "previous_balance_sheet_set = False\n",
    "balance_sheet_final = None\n",
    "\n",
    "previous_cashflow_statement_set = False\n",
    "cashflow_statement_final = None\n",
    "\n",
    "for accession_no in accession_numbers[0:9]: # doesn't work with filings filed before 2017\n",
    "\n",
    "    # get XBRL-JSON of 10-Q or 10-K filing by accession number\n",
    "    xbrl_json_data = get_xbrl_json(accession_no)\n",
    "    \n",
    "    # convert XBRL-JSON to a pandas dataframe\n",
    "    income_statement_uncleaned = get_income_statement(xbrl_json_data)\n",
    "    balance_sheet_uncleaned = get_balance_sheet(xbrl_json_data)\n",
    "    cashflow_statement_uncleaned = get_cash_flow_statement(xbrl_json_data)\n",
    "    \n",
    "    # clean the data\n",
    "#     income_statement_cleaned = clean_data_statement(income_statement_uncleaned)\n",
    "#     balance_sheet_cleaned = clean_data_statement(balance_sheet_uncleaned)\n",
    "#     cashflow_statement_cleaned = clean_data_statement(cashflow_statement_uncleaned)\n",
    "    \n",
    "    # merge new income statement with previously generated income statement\n",
    "    if previous_income_statement_set and previous_balance_sheet_set and previous_cashflow_statement_set:\n",
    "        income_statement_final = merge_statements(income_statement_final, income_statement_uncleaned)\n",
    "        balance_sheet_final = merge_statements(balance_sheet_final, balance_sheet_uncleaned)\n",
    "        cashflow_statement_final = merge_statements(cashflow_statement_final, cashflow_statement_uncleaned)\n",
    "    else:\n",
    "        income_statement_final = income_statement_uncleaned\n",
    "        balance_sheet_final = balance_sheet_uncleaned\n",
    "        cashflow_statement_final = cashflow_statement_uncleaned\n",
    "        previous_income_statement_set = True\n",
    "        previous_balance_sheet_set = True\n",
    "        previous_cashflow_statement_set = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36342583",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_statement_final.to_csv(f\"{ticker}_income_statement.csv\")\n",
    "balance_sheet_final.to_csv(f\"{ticker}_balance_sheet.csv\")\n",
    "cashflow_statement_final.to_csv(f\"{ticker}_cashflow_statement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cc6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936e915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307bfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from sec_api import QueryApi\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb102bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEC_API:\n",
    "    def __init__(self, ticker):\n",
    "        self.API_KEY = \"f9170d7223a9d2ba983443847320aa658edcc210742ee811ca09e1ab0dd956c9\"\n",
    "        \n",
    "        query_api = QueryApi(api_key=self.API_KEY)\n",
    "\n",
    "        # fetch all 10-Q and 10-K\n",
    "        query = { \n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": \"(formType:\\\"10-Q\\\" OR formType:\\\"10-K\\\") AND ticker:\"+ticker\n",
    "                }\n",
    "            },\n",
    "            \"from\": \"0\",\n",
    "            \"size\": \"20\",\n",
    "            \"sort\": [{ \"filedAt\": { \"order\": \"desc\" }}]\n",
    "        }\n",
    "\n",
    "        query_result = query_api.get_filings(query)\n",
    "    \n",
    "        self.accession_numbers = []\n",
    "        # extract accession numbers of each filing\n",
    "        for filing in query_result['filings']:\n",
    "            self.accession_numbers.append(filing['accessionNo'])\n",
    "    \n",
    "    # get XBRL-JSON for a given accession number\n",
    "    def get_xbrl_json(self, accession_no):\n",
    "        '''\n",
    "        Helper function to get XBRL-JSON version of a filing by providing its accession number\n",
    "        '''\n",
    "        # XBRL-to-JSON converter API endpoint\n",
    "        xbrl_converter_api_endpoint = \"https://api.sec-api.io/xbrl-to-json\"\n",
    "\n",
    "        request_url = xbrl_converter_api_endpoint + \"?accession-no=\" + accession_no + \"&token=\" + API_KEY\n",
    "\n",
    "        response_tmp = requests.get(request_url)\n",
    "\n",
    "        return json.loads(response_tmp.text)\n",
    "    \n",
    "    # merge two statements into one statement\n",
    "    # row indices of both statements have to be the same\n",
    "    # statement_b represents the most recent statement.\n",
    "    def merge_statements(self, statement_a, statement_b):\n",
    "        return statement_a.merge(statement_b,\n",
    "                                 how=\"outer\",\n",
    "                                 left_index=True,\n",
    "                                 right_index=True,\n",
    "                                 suffixes=('_left', ''))\n",
    "    \n",
    "    # clean statement\n",
    "    # drop duplicate columns (= column name ends with \"_left\"), drop key_0 column, drop columns with +5 NaNs\n",
    "    def clean_data_statement(self, statement):\n",
    "        for column in statement:\n",
    "\n",
    "            #column has more than 5 NaN values\n",
    "            is_nan_column = statement[column].isna().sum() > 5\n",
    "\n",
    "            if column.endswith('_left') or column == 'key_0' or is_nan_column:\n",
    "                statement = statement.drop(column, axis=1)\n",
    "\n",
    "        # rearrange columns so that first column represents first quarter\n",
    "        # e.g. 2018, 2019, 2020 - and not 2020, 2019, 2018\n",
    "        sorted_columns = sorted(statement.columns.values)\n",
    "\n",
    "        return statement[sorted_columns]\n",
    "    \n",
    "    # convert XBRL-JSON of income statement to pandas dataframe\n",
    "    def get_income_statement(self, xbrl_json):\n",
    "        income_statement_store = {}\n",
    "\n",
    "        # iterate over each US GAAP item in the income statement\n",
    "        for usGaapItem in xbrl_json['StatementsOfIncome']:\n",
    "            values = []\n",
    "            indicies = []\n",
    "\n",
    "            for fact in xbrl_json['StatementsOfIncome'][usGaapItem]:\n",
    "                # only consider items without segment.\n",
    "                if 'segment' not in fact:\n",
    "                    index = fact['period']['startDate'] + '-' + fact['period']['endDate']\n",
    "                    # ensure no index duplicates are created\n",
    "                    if index not in indicies:\n",
    "                        values.append(fact['value'])\n",
    "                        indicies.append(index)\n",
    "\n",
    "            income_statement_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "\n",
    "        income_statement = pd.DataFrame(income_statement_store)\n",
    "        # switch columns and rows so that US GAAP items are rows and each column header represents a data range\n",
    "        return income_statement.T\n",
    "    \n",
    "    # convert XBRL-JSON of balance sheet to pandas dataframe\n",
    "    def get_balance_sheet(self, xbrl_json):\n",
    "        balance_sheet_store = {}\n",
    "\n",
    "        for usGaapItem in xbrl_json['BalanceSheets']:\n",
    "            values = []\n",
    "            indicies = []\n",
    "\n",
    "            for fact in xbrl_json['BalanceSheets'][usGaapItem]:\n",
    "                # only consider items without segment.\n",
    "                if 'segment' not in fact:\n",
    "                    index = fact['period']['instant']\n",
    "\n",
    "                    # avoid duplicate indices with same values\n",
    "                    if index in indicies:\n",
    "                        continue\n",
    "\n",
    "                    # add 0 if value is nil\n",
    "                    if \"value\" not in fact:\n",
    "                        values.append(0)\n",
    "                    else:\n",
    "                        values.append(fact['value'])\n",
    "\n",
    "                    indicies.append(index)\n",
    "\n",
    "                balance_sheet_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "\n",
    "        balance_sheet = pd.DataFrame(balance_sheet_store)\n",
    "        # switch columns and rows so that US GAAP items are rows and each column header represents a date instant\n",
    "        return balance_sheet.T\n",
    "    \n",
    "    # generate the cash flow statement\n",
    "    def get_cash_flow_statement(self, xbrl_json):\n",
    "        cash_flows_store = {}\n",
    "\n",
    "        for usGaapItem in xbrl_json['StatementsOfCashFlows']:\n",
    "            values = []\n",
    "            indicies = []\n",
    "\n",
    "            for fact in xbrl_json['StatementsOfCashFlows'][usGaapItem]:\n",
    "                # only consider items without segment\n",
    "                if 'segment' not in fact:\n",
    "                    #check if data instant or date range is present\n",
    "                    if 'instant' in fact['period']:\n",
    "                        index = fact['period']['instant']\n",
    "                    else:\n",
    "                        index = fact['period']['startDate'] + '-' + fact['period']['endDate']\n",
    "\n",
    "                    # avoid duplicate indicies with same values\n",
    "                    if index in indicies:\n",
    "                        continue\n",
    "\n",
    "                    if \"value\" not in fact:\n",
    "                        values.append(0)\n",
    "                    else:\n",
    "                        values.append(fact['value'])\n",
    "\n",
    "                    indicies.append(index)\n",
    "\n",
    "            cash_flows_store[usGaapItem] = pd.Series(values, index=indicies)\n",
    "\n",
    "        cash_flows = pd.DataFrame(cash_flows_store)\n",
    "        return cash_flows.T\n",
    "    \n",
    "    \n",
    "    def run(self):\n",
    "        previous_income_statement_set = False\n",
    "        self.income_statement_final = None\n",
    "\n",
    "        previous_balance_sheet_set = False\n",
    "        self.balance_sheet_final = None\n",
    "\n",
    "        previous_cashflow_statement_set = False\n",
    "        self.cashflow_statement_final = None\n",
    "\n",
    "        for accession_no in self.accession_numbers[0:9]: # doesn't work with filings filed before 2017\n",
    "\n",
    "            # get XBRL-JSON of 10-Q or 10-K filing by accession number\n",
    "            xbrl_json_data = self.get_xbrl_json(accession_no)\n",
    "\n",
    "            # convert XBRL-JSON to a pandas dataframe\n",
    "            income_statement_uncleaned = self.get_income_statement(xbrl_json_data)\n",
    "            balance_sheet_uncleaned = self.get_balance_sheet(xbrl_json_data)\n",
    "            cashflow_statement_uncleaned = self.get_cash_flow_statement(xbrl_json_data)\n",
    "\n",
    "            # clean the data\n",
    "            income_statement_cleaned = self.clean_data_statement(income_statement_uncleaned)\n",
    "            balance_sheet_cleaned = self.clean_data_statement(balance_sheet_uncleaned)\n",
    "            cashflow_statement_cleaned = self.clean_data_statement(cashflow_statement_uncleaned)\n",
    "\n",
    "            # merge new income statement with previously generated income statement\n",
    "            if previous_income_statement_set and previous_balance_sheet_set and previous_cashflow_statement_set:\n",
    "                self.income_statement_final = self.clean_data_statement(self.merge_statements(self.income_statement_final, income_statement_cleaned))\n",
    "                self.balance_sheet_final = self.clean_data_statement(self.merge_statements(self.balance_sheet_final, balance_sheet_cleaned))\n",
    "                self.cashflow_statement_final = self.clean_data_statement(self.merge_statements(self.cashflow_statement_final, cashflow_statement_cleaned))\n",
    "            else:\n",
    "                self.income_statement_final = income_statement_cleaned\n",
    "                self.balance_sheet_final = balance_sheet_cleaned\n",
    "                self.cashflow_statement_final = cashflow_statement_cleaned\n",
    "                previous_income_statement_set = True\n",
    "                previous_balance_sheet_set = True\n",
    "                previous_cashflow_statement_set = True\n",
    "        \n",
    "    \n",
    "    def save_csv(self):\n",
    "        self.income_statement_final.to_csv(f\"{ticker}_income_statement.csv\")\n",
    "        self.balance_sheet_final.to_csv(f\"{ticker}_balance_sheet.csv\")\n",
    "        self.cashflow_statement_final.to_csv(f\"{ticker}_cashflow_statement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f566e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SEC_API(ticker=\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85e200d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-30c70dc5e6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-86b2522af9b7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# get XBRL-JSON of 10-Q or 10-K filing by accession number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mxbrl_json_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xbrl_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccession_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# convert XBRL-JSON to a pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-86b2522af9b7>\u001b[0m in \u001b[0;36mget_xbrl_json\u001b[0;34m(self, accession_no)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mxbrl_converter_api_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://api.sec-api.io/xbrl-to-json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mrequest_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxbrl_converter_api_endpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"?accession-no=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maccession_no\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"&token=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mresponse_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "sec.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d85bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95032b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae81706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
